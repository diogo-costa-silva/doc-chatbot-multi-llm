# Changelog

## Version 0.6.0 - Production Readiness (2025-10-22)

### üî¥ Critical Fixes

**Legal & Licensing:**
- ‚úÖ **Added MIT LICENSE file** - Repository now has proper open-source licensing
  - Includes copyright notice and full MIT license text
  - Aligns with README.md license declaration

**Documentation Accuracy:**
- ‚úÖ **Fixed URL placeholders** - Replaced hardcoded "your-username" with clear `<YOUR_GITHUB_USERNAME>` variable
  - Updated README.md, DEPLOYMENT.md, CONTRIBUTING.md
  - Added explanatory notes for users to replace placeholders
- ‚úÖ **Removed broken references** - Fixed documentation pointing to non-existent files
  - CLAUDE.md: Updated reference from `README_HUGGINGFACE.md` to `README.md`
  - CLAUDE.md: Updated reference from `README_DEPLOY.md` to `DEPLOYMENT.md`

**Dependency Management:**
- ‚úÖ **Clarified multi-LLM dependency strategy** - Documented local vs cloud approach
  - `pyproject.toml`: Includes ALL LLMs (Gemini, Groq, Ollama) for full local development
  - `requirements.txt`: Cloud-only LLMs (Gemini, Groq) for HF Spaces deployment
  - Added inline comments explaining separation strategy
  - Maintains true "Multi-LLM" promise: `uv sync` gives full offline capabilities

**Security & Robustness:**
- ‚úÖ **Added file size validation** - Prevents crashes from oversized uploads
  - `document_processor.py`: Validates file size before processing
  - TXT files: 10MB limit
  - PDF files: 50MB limit
  - Clear error messages with size limits displayed to users

**Version Control:**
- ‚úÖ **Updated .gitignore for uv.lock** - Prevents version lock conflicts
  - Uncommented `uv.lock` exclusion
  - Added explanatory comment about cross-environment compatibility

---

### üü° Quality Improvements

**Code Quality:**
- ‚úÖ **Replaced print() with structured logging** - Professional error handling
  - Added `logging` module to `llm_manager.py`
  - Replaced 8 print statements with appropriate log levels (error, warning, info)
  - Improves debugging and production monitoring

**Configuration Management:**
- ‚úÖ **Created centralized config module** - Eliminated magic numbers
  - New file: `src/config.py` with all constants
  - Extracted: `CHUNK_SIZE=1000`, `CHUNK_OVERLAP=200`, `MAX_CHUNKS_FOR_QUERY=3`
  - Added: `MAX_FILE_SIZE_TXT=10MB`, `MAX_FILE_SIZE_PDF=50MB`
  - Added: `DEFAULT_LLM_TEMPERATURE=0.7`, `DEFAULT_MAX_TOKENS=2000`
  - Updated all modules to import from config

**Internationalization:**
- ‚úÖ **Translated example document to English** - Consistency with codebase
  - `examples/exemplo_documento.txt` ‚Üí `examples/example_document.txt`
  - Content about Gen AI fully translated
  - Maintains educational quality

**Frontend Architecture:**
- ‚úÖ **Moved inline CSS to separate file** - Better code organization
  - Created `.streamlit/style.css` with all custom styles
  - `streamlit_app.py`: Loads CSS from file with graceful fallback
  - Easier to maintain and customize UI

**Module Structure:**
- ‚úÖ **Improved src/__init__.py** - Better package interface
  - Added `__version__ = "0.6.0"` and `__author__`
  - Exported main classes: `LLMManager`, `DocumentProcessor`, `ChatHandler`
  - Exported utility functions: `is_huggingface_space`, `is_ollama_supported`
  - Added `__all__` for explicit public API
  - Enables cleaner imports: `from src import LLMManager`

**Documentation:**
- ‚úÖ **Enhanced packages.txt documentation** - Clear usage guidelines
  - Comprehensive comments explaining purpose
  - Examples of when system packages are needed
  - Format instructions and common package examples
  - Explains why file is currently empty

- ‚úÖ **Reorganized requirements.txt** - Professional dependency management
  - Added clear header and section organization
  - Inline comments for each package explaining purpose
  - Notes section explaining Ollama exclusion
  - Instructions for local development vs HF Spaces

---

### üìä Impact Summary

| Category | Changes |
|----------|---------|
| **Files Created** | 3 (LICENSE, src/config.py, .streamlit/style.css) |
| **Files Modified** | 11 (README, DEPLOYMENT, CONTRIBUTING, CLAUDE, pyproject.toml, .gitignore, streamlit_app, llm_manager, document_processor, __init__, packages.txt, requirements.txt) |
| **Files Renamed** | 1 (exemplo_documento.txt ‚Üí example_document.txt) |
| **Lines Changed** | ~400 |
| **Critical Issues Fixed** | 6 |
| **Quality Improvements** | 7 |

---

### üéØ Deployment Status

**Before v0.6.0:**
- ‚ùå No LICENSE file
- ‚ùå Broken documentation references
- ‚ùå Hardcoded placeholders in docs
- ‚ùå No file upload size limits (crash risk)
- ‚ùå Print statements instead of logging
- ‚ùå Magic numbers scattered in code
- ‚ùå Inline CSS mixed with Python
- ‚ö†Ô∏è Conflicting Ollama dependencies

**After v0.6.0:**
- ‚úÖ **Production-ready for GitHub**
- ‚úÖ **Production-ready for Hugging Face Spaces**
- ‚úÖ **Professional code quality**
- ‚úÖ **Complete documentation**
- ‚úÖ **Robust error handling**
- ‚úÖ **Maintainable architecture**

---

## Version 0.5.0 - Pre-Deployment Preparation (2025-10-21)

### üìö Documentation Overhaul

**Complete English Translation:**
- ‚úÖ All Python source files translated to English (docstrings, comments)
- ‚úÖ Streamlit UI fully translated to English (buttons, labels, messages)
- ‚úÖ `.env.example` comments translated

**New Comprehensive Documentation:**
- ‚úÖ **README.md**: Complete rewrite with clear structure
  - Quick start guide for HF Spaces users
  - Local development setup instructions
  - Project structure and architecture
  - Multi-platform support table
  - Use cases and features overview
- ‚úÖ **docs/DEPLOYMENT.md**: Consolidated deployment guide
  - One-time setup instructions
  - GitHub Actions workflow details
  - Secrets configuration (GitHub & HF)
  - Troubleshooting section
  - Platform differences explained
- ‚úÖ **docs/CONTRIBUTING.md**: Contribution guidelines
  - Code of conduct
  - Development setup
  - Code style guidelines (PEP 8, docstrings, type hints)
  - Commit message format
  - Pull request process
  - Testing checklist

**Documentation Cleanup:**
- ‚ùå Removed duplicate/outdated docs:
  - `docs/PROXIMOS_PASSOS.md`
  - `docs/PROXIMOS_PASSOS_GH_HF.md`
  - `docs/GUIA_RAPIDO.md`
  - `docs/README_DEPLOY.md` (consolidated into DEPLOYMENT.md)
  - `docs/prompt.md`
  - `.github/SETUP_SECRETS.md` (consolidated into DEPLOYMENT.md)
- ‚ùå Removed test file: `txt.txt`
- ‚úÖ Kept: `docs/PARA_CANDIDATURA.md` (user will manage), `docs/prompt_2.md`

### üíª Code Improvements

**Translation to English:**
- All module docstrings and comments
- All function/class documentation
- All UI strings (Streamlit interface)
- Error messages and user feedback
- Configuration helpers and tooltips

**Files Translated:**
- `src/llm_manager.py`
- `src/document_processor.py`
- `src/chat_handler.py`
- `src/platform_utils.py`
- `streamlit_app.py`

**Code Quality:**
- Following Python/Streamlit conventions
- Clear, descriptive variable names
- Comprehensive docstrings with Args/Returns
- Type hints for function signatures

### üìù Dependency Management

**requirements.txt:**
- Added clear section comments
- Organized by category (Web Framework, LLM Providers, etc.)
- Added inline comments for each package
- Noted Ollama exclusion (local-only)

**packages.txt:**
- Clarified purpose (HF Spaces system packages)
- Explained why currently empty
- Added examples for future reference
- Clear English documentation

### üóëÔ∏è Project Cleanup

**Removed Files:**
- `txt.txt` - test file
- `docs/prompt.md` - no longer needed
- `docs/PROXIMOS_PASSOS.md` - superseded
- `docs/PROXIMOS_PASSOS_GH_HF.md` - superseded
- `docs/GUIA_RAPIDO.md` - content integrated into README
- `docs/README_DEPLOY.md` - consolidated into DEPLOYMENT.md
- `.github/SETUP_SECRETS.md` - consolidated into DEPLOYMENT.md

### üéØ Deployment Readiness

**Ready for:**
- ‚úÖ Public GitHub repository
- ‚úÖ Hugging Face Spaces deployment
- ‚úÖ Open-source contributions
- ‚úÖ Global audience (English documentation)
- ‚úÖ Professional presentation

**Repository Structure:**
```
genai-doc-chatbot/
‚îú‚îÄ‚îÄ README.md                    ‚ú® NEW (comprehensive English)
‚îú‚îÄ‚îÄ CHANGELOG.md                 üìù UPDATED (this entry)
‚îú‚îÄ‚îÄ CLAUDE.md                    ‚úÖ (unchanged)
‚îú‚îÄ‚îÄ .env.example                 üåç (English)
‚îú‚îÄ‚îÄ requirements.txt             ‚úÖ (validated, commented)
‚îú‚îÄ‚îÄ packages.txt                 ‚úÖ (validated, documented)
‚îú‚îÄ‚îÄ streamlit_app.py             üåç (English)
‚îú‚îÄ‚îÄ src/                         üåç (all English)
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md            ‚ú® NEW (consolidated guide)
‚îÇ   ‚îú‚îÄ‚îÄ CONTRIBUTING.md          ‚ú® NEW (guidelines)
‚îÇ   ‚îú‚îÄ‚îÄ PARA_CANDIDATURA.md      ‚úÖ (kept, Portuguese)
‚îÇ   ‚îî‚îÄ‚îÄ prompt_2.md              ‚úÖ (kept, user manages)
‚îî‚îÄ‚îÄ .github/workflows/           ‚úÖ (unchanged)
```

### üìä Impact Summary

| Category | Before | After |
|----------|--------|-------|
| **Language** | Mixed PT/EN | ‚úÖ Fully English |
| **Documentation** | 7+ scattered files | ‚úÖ 3 clear, focused docs |
| **Code comments** | Portuguese | ‚úÖ English |
| **UI strings** | Portuguese | ‚úÖ English |
| **Dependencies** | Uncommented list | ‚úÖ Organized with comments |
| **Project structure** | Cluttered | ‚úÖ Clean, professional |

### üöÄ Next Steps

After this version:
1. Push to GitHub repository
2. Configure GitHub secrets for HF deployment
3. Create Hugging Face Space
4. Test automatic deployment workflow
5. Verify application works on HF Spaces
6. Share with global audience

---

## Version 0.4.0 - Toggle de Tema Dark/Light (2025-10-21)

### ‚ú® Nova Funcionalidade: Altern√¢ncia de Tema Vis√≠vel

#### Motiva√ß√£o
- App estava em modo claro por padr√£o (cansativo para a vista)
- Menu nativo do Streamlit (‚öôÔ∏è > Settings > Theme) n√£o √© intuitivo
- Necessidade de toggle vis√≠vel e acess√≠vel no frontend

#### Solu√ß√£o Implementada

**Toggle Visual no Header:**
- Bot√£o no canto superior direito da √°rea principal
- üåú quando em modo escuro (representa "night mode")
- üåû quando em modo claro (representa "day mode")
- Tooltip: "Alternar tema claro/escuro"

**Comportamento:**
- Tema escuro como padr√£o inicial
- Clique alterna instantaneamente entre temas
- Prefer√™ncia persiste durante a sess√£o
- Auto-rerun para aplicar mudan√ßas

#### Decis√µes de UX

**Posicionamento:**
- Header principal (n√£o na sidebar)
- Separado das configura√ß√µes do chatbot (LLM/documentos)
- M√°xima visibilidade, sempre acess√≠vel

**L√≥gica do √çcone:**
- ‚úÖ √çcone representa o **tema atual** (n√£o o destino)
- Segue conven√ß√µes de apps modernas (GitHub, VS Code, Twitter)
- Intuitivo: v√™ lua ‚Üí est√° escuro, v√™ sol ‚Üí est√° claro

#### Implementa√ß√£o T√©cnica

**Ficheiros modificados:**

1. **`.streamlit/config.toml`:**
   - Tema escuro como base padr√£o
   - Cores: `backgroundColor: #0E1117`, `textColor: #FAFAFA`

2. **`streamlit_app.py`:**
   - `initialize_session_state()`: Dicion√°rios com configs dos 2 temas
   - `toggle_theme()`: Aplica tema via `st._config.set_option()` + rerun
   - `main_chat()`: Layout com `st.columns([0.85, 0.15])` para header + toggle

**Estado em `session_state`:**
```python
theme = {
    "current": "dark",  # ou "light"
    "refreshed": True,
    "dark": {...},      # config tema escuro
    "light": {...}      # config tema claro
}
```

#### Corre√ß√£o de L√≥gica
- **Vers√£o inicial (incorreta)**: Modo escuro mostrava üåû
- **Corrigido**: Modo escuro mostra üåú (representa estado atual)

### üìä Resultado

| Aspeto | Antes | Agora |
|--------|-------|-------|
| **Tema padr√£o** | Claro (cansativo) | ‚úÖ Escuro (confort√°vel) |
| **Alternar tema** | Menu escondido ‚öôÔ∏è | ‚úÖ Toggle vis√≠vel no header |
| **UX** | N√£o intuitivo | ‚úÖ √çcone representa estado atual |
| **Localiza√ß√£o** | Misturado com configs | ‚úÖ Separado (header principal) |
| **Persist√™ncia** | Browser | ‚úÖ Sess√£o (session_state) |

## Vers√£o 0.3.1 - Corre√ß√£o L√≥gica API Keys (2025-10-21)

### üêõ Bug Corrigido: Listagem sem API Key

#### Problema
App mostrava lista de modelos Gemini/Groq mesmo sem API key configurada, mas n√£o conseguia us√°-los. Criava falsa expectativa.

#### Corre√ß√£o
- ‚ùå SEM API key: N√£o mostra modelos, exibe erro claro
- ‚úÖ COM API key: Busca modelos dinamicamente via API
- ‚úÖ Valida√ß√£o ocorre ANTES de mostrar op√ß√µes
- ‚úÖ Mensagem clara de como configurar

#### Interface Melhorada
**Quando SEM API key:**
```
‚ùå API key GEMINI necess√°ria para usar este provider!

Como configurar:
1. Obtenha API key gratuita (ver links abaixo)
2. Adicione ao ficheiro .env: GEMINI_API_KEY=sua_chave_aqui
3. Reinicie a aplica√ß√£o

Modelo: [‚ö†Ô∏è Configure GEMINI_API_KEY primeiro]
```

**Quando COM API key no .env:**
```
‚úÖ GEMINI_API_KEY configurada no .env
Primeiros caracteres: AIzaSyA626z11CR...
‚òê Usar outra API key temporariamente
```

## Vers√£o 0.3.0 - Busca Din√¢mica de Modelos (2025-10-21)

### ‚ú® Nova Funcionalidade: Listagem Din√¢mica de Modelos

#### Problema Original
- Erro 404: `gemini-1.5-pro-latest is not found`
- Modelos hard-coded ficavam desatualizados
- Gemini 1.5 descontinuado, substitu√≠do por 2.0 e 2.5

#### Solu√ß√£o Implementada

**Busca Autom√°tica via API:**
- ‚úÖ **Gemini**: `genai.list_models()` busca modelos dispon√≠veis
- ‚úÖ **Groq**: `client.models.list()` busca modelos dispon√≠veis
- ‚úÖ **Ollama**: J√° funcionava (mantido)

**Funcionalidades:**
1. Busca modelos direto da API quando API key dispon√≠vel
2. Cache de resultados para performance
3. Bot√£o "üîÑ Atualizar Lista" para refresh manual
4. Fallback para lista padr√£o se API falhar
5. Indicador visual: "‚úÖ X modelo(s) dispon√≠vel(is) via API"

**Modelos Atualizados (Gemini):**
- ‚ùå `gemini-1.5-pro-latest` (removido - descontinuado)
- ‚ùå `gemini-1.5-flash-latest` (removido - descontinuado)
- ‚úÖ `gemini-2.5-flash-lite` (novo - r√°pido, baixo custo)
- ‚úÖ `gemini-2.5-flash` (novo - est√°vel)
- ‚úÖ `gemini-2.5-pro` (novo - melhor qualidade)
- ‚úÖ `gemini-2.0-flash-exp` (mantido - experimental)

### üìä Benef√≠cios

| Aspeto | Antes (v0.2.2) | Agora (v0.3.0) |
|--------|----------------|----------------|
| **Modelos** | Hard-coded, desatualizados | ‚úÖ Busca din√¢mica da API |
| **Erros 404** | ‚ùå Poss√≠vel | ‚úÖ Imposs√≠vel |
| **Novos modelos** | Precisa atualizar c√≥digo | ‚úÖ Aparecem automaticamente |
| **Valida√ß√£o API Key** | S√≥ no uso | ‚úÖ Valida ao listar modelos |
| **Performance** | N/A | ‚úÖ Cache de resultados |

### üîß Implementa√ß√£o T√©cnica

**Novas fun√ß√µes em `llm_manager.py`:**
```python
get_gemini_models_dynamic(api_key) -> (modelos, sucesso)
get_groq_models_dynamic(api_key) -> (modelos, sucesso)
```

**Atualiza√ß√£o em `streamlit_app.py`:**
- Deteta API key do `.env` automaticamente
- Busca modelos com spinner de loading
- Cache em `session_state` para evitar chamadas repetidas
- Bot√£o refresh por provider

## Vers√£o 0.2.2 - Corre√ß√£o KeyError + Rename (2025-10-21)

### üêõ Bug Corrigido: KeyError no Session State

#### Problema
```
KeyError: 'st.session_state has no key "$WIDGET_ID-..."'
```
- **Causa**: Valida√ß√£o cont√≠nua modificava `session_state` antes dos widgets renderizarem
- **Efeito**: App crashava ao carregar

#### Corre√ß√£o
- Uso de `st.session_state.get()` com defaults seguros
- Valida√ß√£o n√£o causa side-effects antes dos widgets
- Flag `_needs_revalidation` para desconfigurar ap√≥s widgets criados

### üìù Renomea√ß√£o: app.py ‚Üí streamlit_app.py

#### Por que mudar?
- ‚úÖ **Conven√ß√£o oficial** do Streamlit
- ‚úÖ **Hugging Face** reconhece automaticamente
- ‚úÖ **Streamlit Cloud** detecta sem configura√ß√£o
- ‚úÖ Comando mais simples: `streamlit run .`

#### Ficheiros atualizados
- `app.py` ‚Üí `streamlit_app.py`
- Toda a documenta√ß√£o (5 ficheiros .md)
- README, GUIA_RAPIDO, PROXIMOS_PASSOS, etc.

## Vers√£o 0.2.1 - Corre√ß√£o Cr√≠tica Ollama (2025-10-21)

### üêõ Bug Cr√≠tico Corrigido

#### Problema: API Ollama mudou e parou de funcionar
- **Causa**: M√≥dulo `ollama` Python mudou estrutura de retorno em vers√£o recente
- **Sintoma**: `get_ollama_models()` falhava com `KeyError: 'name'`
- **Impacto**: App mostrava "Ollama n√£o est√° a correr" mesmo com servi√ßo ativo

#### Corre√ß√£o Implementada
```python
# ANTES (n√£o funcionava):
models = ollama.list()
installed = [model['name'].split(':')[0] for model in models['models']]

# AGORA (corrigido):
result = ollama.list()  # Retorna objeto ListResponse
for model in result.models:  # .models √© lista de objetos Model
    model_name = model.model  # Usar .model em vez de ['name']
```

### ‚úÖ Valida√ß√£o Cont√≠nua de Estado

#### Problema: Estado inconsistente
- **Antes**: Badge dizia "Configurado" mas Ollama n√£o conectava
- **Causa**: Estado `llm_configured` guardado mas nunca re-validado

#### Solu√ß√£o
- Valida√ß√£o autom√°tica em cada render para provider Ollama
- Auto-desconfigura se conex√£o for perdida
- Aviso espec√≠fico: "‚ö†Ô∏è Conex√£o perdida! Verifique o servi√ßo Ollama."

### üìä Resultado

| Situa√ß√£o | Antes | Agora |
|----------|-------|-------|
| Ollama ativo | ‚ùå "n√£o est√° a correr" | ‚úÖ "ativo - 12 modelos" |
| Estado badge | ‚úÖ verde (mas falso) | ‚úÖ verde (validado) |
| Ollama cai | ‚úÖ verde (inconsistente) | ‚ö†Ô∏è amarelo + aviso |
| Re-conecta | Manual reconfig | ‚úÖ Auto-deteta |

## Vers√£o 0.2.0 - Melhorias de UX (2025-10-21)

### üé® Melhorias de Interface

#### Indicador de Estado Sempre Vis√≠vel
- **Antes**: N√£o era claro se o LLM estava configurado
- **Agora**: Badge verde "‚úÖ LLM Configurado e Pronto!" no topo da sidebar
- **Benef√≠cio**: Utilizador sabe sempre o estado da aplica√ß√£o

#### Bot√£o "Configurar LLM" Melhorado
- **Antes**: Bot√£o n√£o dava feedback claro, confuso se funcionou
- **Agora**:
  - Muda para "üîÑ Reconfigurar LLM" quando j√° configurado
  - Mostra bal√µes animados quando sucesso
  - Mensagens de erro espec√≠ficas por provider
  - Fica desativado se falta API key
- **Benef√≠cio**: Feedback visual claro e imediato

#### Dete√ß√£o de Modelos Ollama
- **Antes**: Dizia "nenhum modelo instalado" mesmo com modelos presentes
- **Agora**:
  - Deteta corretamente se Ollama est√° a correr
  - Lista modelos instalados quando ativo
  - Mensagens espec√≠ficas: "Ollama n√£o est√° a correr" vs "Nenhum modelo instalado"
  - Bot√£o "üîÑ Atualizar Lista" para refresh manual
- **Benef√≠cio**: Diagn√≥stico claro de problemas

#### Informa√ß√£o de Configura√ß√£o Atual
- **Novo**: Mostra provider e modelo em uso: "üì° Usando: OLLAMA - llama3.2"
- **Benef√≠cio**: Utilizador sabe exatamente qual LLM est√° a usar

### üîß Melhorias T√©cnicas

#### LLMManager
- Nova fun√ß√£o `get_ollama_models()` retorna tupla `(modelos, est√°_ativo)`
- Nova fun√ß√£o `check_ollama_status()` para verificar servi√ßo
- Melhor error handling em todas as opera√ß√µes Ollama

#### App.py
- Estado da aplica√ß√£o mais robusto
- Valida√ß√£o de API keys antes de permitir configura√ß√£o
- Mensagens de erro contextualizadas por provider

### üìã Problemas Corrigidos

1. ‚úÖ Ollama n√£o detetava modelos instalados
2. ‚úÖ Bot√£o "Configurar LLM" sem feedback
3. ‚úÖ N√£o era claro quando LLM estava pronto
4. ‚úÖ Erro gen√©rico sem informa√ß√£o √∫til

### üéØ Melhorias de UX em Resumo

| Aspeto | Antes | Agora |
|--------|-------|-------|
| **Estado do LLM** | Invis√≠vel | Badge sempre vis√≠vel no topo |
| **Feedback ao configurar** | Nenhum | Bal√µes + mensagem + rerun |
| **Dete√ß√£o Ollama** | Falhava silenciosamente | Diagn√≥stico claro do problema |
| **Bot√£o configurar** | Sempre "Configurar" | Muda para "Reconfigurar" |
| **Erro sem API key** | Permitia clicar | Bot√£o desativado |
| **Info atual** | Nenhuma | Mostra provider/modelo em uso |

## Vers√£o 0.1.0 - Release Inicial (2025-10-21)

### Funcionalidades
- Suporte multi-LLM (Gemini, Groq, Ollama)
- Processamento de documentos TXT/PDF
- Chat com streaming
- Interface Streamlit
- Exporta√ß√£o de conversas
- Documenta√ß√£o completa
